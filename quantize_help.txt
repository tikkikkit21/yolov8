usage: preprocess.py [-h] --input INPUT --output OUTPUT
                     [--skip_optimization SKIP_OPTIMIZATION]
                     [--skip_onnx_shape SKIP_ONNX_SHAPE]
                     [--skip_symbolic_shape SKIP_SYMBOLIC_SHAPE]
                     [--auto_merge] [--int_max INT_MAX] [--guess_output_rank]
                     [--verbose VERBOSE] [--save_as_external_data]
                     [--all_tensors_to_one_file]
                     [--external_data_location EXTERNAL_DATA_LOCATION]
                     [--external_data_size_threshold EXTERNAL_DATA_SIZE_THRESHOLD]

Model optimizer and shape inferencer, in preparation for quantization,
Consists of three optional steps: 1. Symbolic shape inference (best for
transformer models). 2. Model optimization. 3. ONNX shape inference. Model
quantization with QDQ format, i.e. inserting QuantizeLinear/DeQuantizeLinear
on the tensor, requires tensor shape information to perform its best.
Currently, shape inferencing works best with optimized model. As a result, it
is highly recommended to run quantization on optimized model with shape
information. This is the tool for optimization and shape inferencing.
Essentially this tool performs the following three (skippable) steps: 1.
Symbolic shape inference. 2. Model optimization 3. ONNX shape inference

optional arguments:
  -h, --help            show this help message and exit
  --input INPUT         Path to the input model file
  --output OUTPUT       Path to the output model file
  --skip_optimization SKIP_OPTIMIZATION
                        Skip model optimization step if true. It's a known
                        issue that ORT optimization has difficulty with model
                        size greater than 2GB, rerun with this option to get
                        around this issue.
  --skip_onnx_shape SKIP_ONNX_SHAPE
                        Skip ONNX shape inference. Symbolic shape inference is
                        most effective with transformer based models. Skipping
                        all shape inferences may reduce the effectiveness of
                        quantization, as a tensor with unknown shape can not
                        be quantized.
  --skip_symbolic_shape SKIP_SYMBOLIC_SHAPE
                        Skip symbolic shape inference. Symbolic shape
                        inference is most effective with transformer based
                        models. Skipping all shape inferences may reduce the
                        effectiveness of quantization, as a tensor with
                        unknown shape can not be quantized.
  --auto_merge          Automatically merge symbolic dims when confliction
                        happens
  --int_max INT_MAX     maximum value for integer to be treated as boundless
                        for ops like slice
  --guess_output_rank   guess output rank to be the same as input 0 for
                        unknown ops
  --verbose VERBOSE     Prints detailed logs of inference, 0: turn off, 1:
                        warnings, 3: detailed
  --save_as_external_data
                        Saving an ONNX model to external data
  --all_tensors_to_one_file
                        Saving all the external data to one file
  --external_data_location EXTERNAL_DATA_LOCATION
                        The file location to save the external file
  --external_data_size_threshold EXTERNAL_DATA_SIZE_THRESHOLD
                        The size threshold for external data
